{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba26052c-2bb9-48c9-9565-ed139cc65688",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiment 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73029e5-94b9-4123-93c9-1570f19d7a84",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training on Google Collab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aOKaMgUnZYbP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOKaMgUnZYbP",
    "outputId": "240f59de-2be0-483f-d1db-521468987b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount your google drive in google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091283c-4f2b-4c79-992b-690f01776638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to base directory\n",
    "%cd /content/drive/MyDrive/University/Master-Thesis/Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d466b3b-c927-4ef1-ac65-ec3db4576da3",
   "metadata": {},
   "source": [
    "### Training on Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93a4765-f570-4a09-a4bb-92361eec9f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original working directory is /Users/jankreischer/Library/Mobile Documents/com~apple~CloudDocs/Master-Thesis/Code/experiments/jans_experiment_01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "original_working_directory_path = os.getcwd()\n",
    "print(\"The original working directory is {0}\".format(os.getcwd()))\n",
    "\n",
    "def to_original_working_directory():\n",
    "    os.chdir(original_working_directory_path)\n",
    "    print(f\"Changed to original working directory {original_working_directory_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f290fa7-29f1-4376-86e4-56d78679f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_root_working_directory():\n",
    "    root_working_directory_path = os.path.join(original_working_directory_path, \"../..\")\n",
    "    os.chdir(root_working_directory_path)\n",
    "    print(f\"Changed to root working directory {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f949990-e666-4f91-a9f4-831dc7b2462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to root working directory /Users/jankreischer/Library/Mobile Documents/com~apple~CloudDocs/Master-Thesis/Code\n"
     ]
    }
   ],
   "source": [
    "to_root_working_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0887f8f1-2b34-4aeb-b56e-a7da253ca19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9c63e331-b856-42fc-a6c2-7ceb5d3a99d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = None\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55107adb-da39-4ef0-b80c-1988f77cb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_random():\n",
    "    random.seed(42)\n",
    "    torch.random.manual_seed(42)\n",
    "    np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "03dc6a8b-3b21-43eb-9627-5bd2a3556f6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YourFavoriteNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [210]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Move your model to mps just like any other device\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYourFavoriteNet\u001b[49m()\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mto(mps_device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Now every call runs on the GPU\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YourFavoriteNet' is not defined"
     ]
    }
   ],
   "source": [
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "\n",
    "    # Create a Tensor directly on the mps device\n",
    "    x = torch.ones(5, device=mps_device)\n",
    "    # Or\n",
    "    x = torch.ones(5, device=\"mps\")\n",
    "\n",
    "    # Any operation happens on the GPU\n",
    "    y = x * 2\n",
    "\n",
    "    # Move your model to mps just like any other device\n",
    "    model = YourFavoriteNet()\n",
    "    model.to(mps_device)\n",
    "\n",
    "    # Now every call runs on the GPU\n",
    "    pred = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80523a04-4e7d-47de-8c6f-4d0c6c77d05a",
   "metadata": {},
   "source": [
    "### 1. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2b2e7b-5fad-4d40-b163-163fc1e4eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Dependencies\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63770f25-75fb-4b9d-8ea3-51354c8152dd",
   "metadata": {
    "id": "63770f25-75fb-4b9d-8ea3-51354c8152dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jankreischer/opt/anaconda3/envs/FedRL-for-IT-Sec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Global Dependencies\n",
    "from src.custom_types import Behavior, MTDTechnique\n",
    "from src.data_provider import DataProvider\n",
    "from src.agent import Agent\n",
    "#from src.simulation_engine import SimulationEngine\n",
    "from src.evaluation_utils import plot_learning, seed_random, get_pretrained_agent, evaluate_agent_on_afterstates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5925311-762e-4c72-aac6-bebdc12dc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def evaluate_agent(agent: Agent, test_data):\n",
    "    # check predictions with learnt dqn\n",
    "    agent.online_net.eval()\n",
    "    res_dict = {}\n",
    "    objective_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for b, d in test_data.items():\n",
    "            if b != Behavior.NORMAL:\n",
    "                cnt_corr = 0\n",
    "                cnt = 0\n",
    "                for state in d:\n",
    "                    action = agent.take_greedy_action(state[:-1])\n",
    "                    if b in supervisor_map[action]:\n",
    "                        cnt_corr += 1\n",
    "                    cnt += 1\n",
    "                res_dict[b] = (cnt_corr, cnt)\n",
    "\n",
    "            for i in range(len(actions)):\n",
    "                if b in supervisor_map[i]:\n",
    "                    objective_dict[b] = actions[i]\n",
    "    labels = (\"Behavior\", \"Accuracy\", \"Objective\")\n",
    "    results = []\n",
    "\n",
    "    for b, t in res_dict.items():\n",
    "        results.append((b.value, f'{(100 * t[0] / t[1]):.2f}%', objective_dict[b].value))\n",
    "    print(tabulate(results, headers=labels, tablefmt=\"orgtbl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb330a90-6693-4783-b9da-593e676753d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(title, returns, epsilons):\n",
    "    #x = range(1, len(returns)+1)\n",
    "    x = [i + 1 for i in range(N_EPISODES)]\n",
    "    fig = plt.figure()\n",
    "    #fig.title(title)\n",
    "    \n",
    "    ax = fig.add_subplot(111, label=\"1\")\n",
    "    ax2 = fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Episode\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(returns)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(returns[max(0, t - 20):(t + 1)])\n",
    "\n",
    "    ax2.scatter(x, running_avg, color=\"C1\", s=2 ** 2)\n",
    "    # ax2.xaxis.tick_top()\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    # ax2.set_xlabel('x label 2', color=\"C1\")\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    # ax2.xaxis.set_label_position('top')\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    # ax2.tick_params(axis='x', colors=\"C1\")\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    #plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d998e-e35a-4cf8-9681-fb503d077742",
   "metadata": {},
   "source": [
    "### 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed9b565-9e46-4af9-b0e3-30355a6a6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "\n",
    "# Alternatively two classes ServerHyperparameter and ClientHyperparameter\n",
    "\n",
    "class Hyperparameter:\n",
    "    # Server Params\n",
    "    NR_CLIENTS = 5\n",
    "    NR_TRAINING_ROUNDS = 2\n",
    "    # The number of epochs per training round is determined implicitely\n",
    "    \n",
    "    # Client Params\n",
    "    GAMMA = 0.1 #0.99 # discount factor for future rewards\n",
    "    BATCH_SIZE = 100\n",
    "    BUFFER_SIZE = 500\n",
    "    MIN_REPLAY_SIZE = 100\n",
    "    # Epsilon determining Exploration Exploitation Trade Off\n",
    "    EPSILON_START = 1.0\n",
    "    EPSILON_DEC = 1e-4\n",
    "    EPSILON_END = 0.01\n",
    "\n",
    "    TARGET_UPDATE_FREQ = 100\n",
    "    LEARNING_RATE = 1e-4\n",
    "    N_EPISODES = 10000\n",
    "    LOG_FREQ = 100\n",
    "    DIMS = 20\n",
    "    PI = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70abc681-3cfc-434e-838f-d322c79c7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "\n",
    "GAMMA = 0.1 #0.99 # discount factor for future rewards\n",
    "BATCH_SIZE = 100\n",
    "N_EPISODES = 10000\n",
    "\n",
    "NR_ROUNDS = 10\n",
    "NR_EPISODES_PER_ROUND = 1000\n",
    "\n",
    "BUFFER_SIZE = 500\n",
    "MIN_REPLAY_SIZE = 100\n",
    "# Epsilon determining Exploration Exploitation Trade Off\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DEC = 5e-3 # 0.005\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "TARGET_UPDATE_FREQ = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "LOG_FREQ = 100\n",
    "DIMS = 20\n",
    "PI = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac612fb-fbf2-4e11-8824-cb2638258019",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180f9b0-0d46-4d03-bafb-81f31702f505",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Tasks\n",
    "##### Done\n",
    "- Why are we not using softmax in the last layer of the agent? [o]\n",
    "- Check timos epsilon decay strategy [✓]\n",
    "- Introduce verbosity levels [✓]\n",
    "\n",
    "##### Open\n",
    "- Check the format of training data\n",
    "- Validate the copying of the parameters works properly\n",
    "- How often do i have to initialze the replay memory?\n",
    "- Make sure that the weights of the DQN are not being reset by the training\n",
    "- If you really want to make this work online then the right aggregation strategy is key. Maybe ensemble models would be good.\n",
    "- Check different federated aggregation strategies for class distinct problems\n",
    "- Parallelize the training\n",
    "- Move the training to the GPU.\n",
    "- Split the dataset presented to each client\n",
    "- Make the threadding more efficient\n",
    "- Add softmax to the last layer\n",
    "- Check how the accuracy/ f1 score was computed by timo\n",
    "- Check different weight aggregation strategies []\n",
    "- Check the weighting strategy that Timo used\n",
    "- Ipmlemtne Multiclass Imbalance Degree (MID) Metric\n",
    "- Print a graph with overall attack detection accuracy (How many got mitigated and how many did not get mitigated)\n",
    "- Add f1 score to the model evaluation output\n",
    "- Check which loss we are using (purpose of binary cross entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1730f41c-8fc6-4b18-9f0d-661ef4692f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List\n",
    "from src.custom_types import Behavior, MTDTechnique, actions, supervisor_map\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# handles the supervised, online-simulation of episodes\n",
    "class SensorEnvironment:\n",
    "\n",
    "    def __init__(self, train_data: Dict[Behavior, np.ndarray] = None):\n",
    "        print(\"Recognized Behaviours\")\n",
    "        print(train_data.keys())\n",
    "        self.train_data = train_data\n",
    "        self.current_state: np.array = None\n",
    "        self.observation_space_size: int = len(self.train_data[Behavior.NORMAL][0][:-1])\n",
    "        self.actions: List[int] = [i for i in range(len(actions))]\n",
    "\n",
    "    # Returns a randomly selected attack state with non normal behaviour.\n",
    "    def sample_random_attack_state(self):\n",
    "        \"\"\"i.e. for starting state of an episode,\n",
    "        (with replacement; it is possible that the same sample is chosen multiple times)\"\"\"\n",
    "        rb = random.choice([b for b in self.train_data.keys() if b != Behavior.NORMAL])\n",
    "        attack_data = self.train_data[rb]\n",
    "        return attack_data[np.random.randint(attack_data.shape[0], size=1), :]\n",
    "\n",
    "    # Return random sample with specified behaviour\n",
    "    def sample_behavior(self, b: Behavior):\n",
    "        behavior_data = self.train_data[b]\n",
    "        return behavior_data[np.random.randint(behavior_data.shape[0], size=1), :]\n",
    "\n",
    "    def step(self, action: int):\n",
    "        current_behavior = self.current_state.squeeze()[-1]\n",
    "\n",
    "        if current_behavior in supervisor_map[action]:\n",
    "            # print(\"correct mtd chosen according to supervisor\")\n",
    "            new_state = self.sample_behavior(Behavior.NORMAL)\n",
    "            reward = self.calculate_reward(True)\n",
    "            isTerminalState = True\n",
    "        else:\n",
    "            # print(\"incorrect mtd chosen according to supervisor\")\n",
    "            new_state = self.sample_behavior(current_behavior)\n",
    "            reward = self.calculate_reward(False)\n",
    "            isTerminalState = False\n",
    "\n",
    "        self.current_state = new_state\n",
    "        return new_state, reward, isTerminalState\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.sample_random_attack_state()\n",
    "        return self.current_state\n",
    "\n",
    "    # TODO: possibly adapt to distinguish between MTDs that are particularly wasteful in case of wrong deployment\n",
    "    def calculate_reward(self, success):\n",
    "        \"\"\"\n",
    "        this method can be refined to distinguish particularly wasteful MTDs (i.e. Dirtrap penalized harder than rootkit sanitization)\n",
    "        \"\"\"\n",
    "        if success:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f479e0d-33cc-4b07-b6be-8125d0df9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims,\n",
    "                 n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Layers\n",
    "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        #self.softmax = torch.nn.Softmax()\n",
    "        # Why are we not using softmax in the last layer\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, agent_id: int, input_dims: int, n_actions, batch_size,\n",
    "                 lr, gamma, epsilon, eps_end=0.02, eps_dec=1e-4, buffer_size=100000):\n",
    "        self.agent_id = agent_id\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "\n",
    "        self.episode_action_memory = set()\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.reward_buffer = deque([0.0], maxlen=100)  # for printing progress\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.online_net = DeepQNetwork(lr, n_actions=n_actions,\n",
    "                                       input_dims=input_dims,\n",
    "                                       fc1_dims=60, fc2_dims=30)\n",
    "        self.target_net = DeepQNetwork(lr, n_actions=n_actions,\n",
    "                                       input_dims=input_dims,\n",
    "                                       fc1_dims=60, fc2_dims=30)\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        try:\n",
    "            if np.random.random() > self.epsilon:\n",
    "                #\n",
    "                action = self.take_greedy_action(observation)\n",
    "                if action in self.episode_action_memory:\n",
    "                    action = np.random.choice(list(set(self.action_space).difference(self.episode_action_memory)))\n",
    "            else:\n",
    "                action = np.random.choice(list(set(self.action_space).difference(self.episode_action_memory)))\n",
    "            self.episode_action_memory.add(action)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "        return action\n",
    "\n",
    "    def take_greedy_action(self, observation):\n",
    "        state = torch.from_numpy(observation.astype(np.float32)).to(self.online_net.device)\n",
    "        actions = self.online_net.forward(state)\n",
    "        action = torch.argmax(actions).item()\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # init data batch from memory replay for dqn\n",
    "        transitions = random.sample(self.replay_buffer, self.batch_size)\n",
    "        b_obses = np.stack([t[0].astype(np.float32).squeeze(0) for t in transitions], axis=0)\n",
    "        b_actions = np.asarray([t[1] for t in transitions]).astype(np.int64)\n",
    "        b_rewards = np.asarray([t[2] for t in transitions]).astype(np.int16)\n",
    "        b_new_obses = np.stack([t[3].astype(np.float32).squeeze(0) for t in transitions], axis=0)\n",
    "        b_dones = np.asarray([t[4] for t in transitions]).astype(np.int16)\n",
    "        t_obses = torch.from_numpy(b_obses).to(self.target_net.device)\n",
    "        t_actions = torch.from_numpy(b_actions).to(self.target_net.device)\n",
    "        t_rewards = torch.from_numpy(b_rewards).to(self.target_net.device)\n",
    "        t_new_obses = torch.as_tensor(b_new_obses).to(self.target_net.device)\n",
    "        t_dones = torch.as_tensor(b_dones).to(self.target_net.device)\n",
    "\n",
    "        # compute targets\n",
    "        target_q_values = self.target_net(t_new_obses)\n",
    "        max_target_q_values = torch.max(target_q_values, dim=1)[0]\n",
    "\n",
    "        targets = (t_rewards + self.gamma * (1 - t_dones) * max_target_q_values).unsqueeze(1)\n",
    "\n",
    "        # compute loss\n",
    "        q_values = self.online_net(t_obses)\n",
    "        taken_action_q_values = torch.gather(input=q_values, dim=1, index=t_actions.unsqueeze(1))\n",
    "\n",
    "        loss = self.online_net.loss(taken_action_q_values, targets).to(self.target_net.device)\n",
    "\n",
    "        # gradient descent\n",
    "        self.online_net.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.online_net.optimizer.step()\n",
    "\n",
    "        # epsilon decay\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "    def get_weights(self):\n",
    "        print(f\"SERVER <--- WEIGHTS --- AGENT {self.agent_id}\")\n",
    "        return copy.deepcopy(self.target_net.state_dict())\n",
    "    \n",
    "    def update_weights(self, model_params):\n",
    "        print(f\"SERVER --- WEIGHTS ---> AGENT {self.agent_id}\")\n",
    "        self.online_net.load_state_dict(copy.deepcopy(model_params))\n",
    "        self.target_net.load_state_dict(copy.deepcopy(model_params))\n",
    "        \n",
    "    '''\n",
    "    def save_agent_state(self, n: int, directory: str):\n",
    "        torch.save({\n",
    "            'online_net_state_dict': self.online_net.state_dict(),\n",
    "            'target_net_state_dict': self.target_net.state_dict(),\n",
    "            'batch_size': self.batch_size,\n",
    "            'replay_buffer': self.replay_buffer,\n",
    "            'reward_buffer': self.reward_buffer,\n",
    "            'action_space': self.action_space,\n",
    "            'gamma': self.gamma,\n",
    "            'eps': self.epsilon,\n",
    "            'eps_min': self.eps_min,\n",
    "            'eps_dec': self.eps_dec,\n",
    "            'lr': self.lr\n",
    "        }, f\"{directory}/trained_models/agent_{n}.pth\")\n",
    "\n",
    "        #torch.save(self.online_net.state_dict(), f\"offline_prototype_2_raw_behaviors/trained_models/online_net_{n}.pth\")\n",
    "        #torch.save(self.target_net.state_dict(), f\"offline_prototype_2_raw_behaviors/trained_models/target_net_{n}.pth\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f782cfd-577e-47e8-8ac0-362b58e91db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import copy\n",
    "#from torch.utils.data import DataLoader as DataProvider\n",
    "\n",
    "\n",
    "class Client:\n",
    "        # memory buffer is influenced by env.step -> resetting to previous action, which may result in unbalanced training\n",
    "    def __init__(self, client_id: int, agent: Agent, environment: SensorEnvironment):\n",
    "        self.client_id = client_id\n",
    "        self.agent = agent\n",
    "        self.environment = environment\n",
    "        self.episode_returns = [] \n",
    "        self.eps_history = []\n",
    "    \n",
    "    '''\n",
    "    test\n",
    "    '''\n",
    "    def init_replay_memory(self, min_size):\n",
    "        obs = self.environment.reset()\n",
    "        episode_action_memory = []\n",
    "        i = 0\n",
    "        while i < min_size:\n",
    "            try:\n",
    "                action = np.random.choice(list({0,1,2,3}.difference(episode_action_memory)))\n",
    "                episode_action_memory.append(action)\n",
    "            except ValueError:\n",
    "                obs = self.environment.reset()\n",
    "                episode_action_memory = []\n",
    "                # results in slightly less entries than min_size\n",
    "                print(\"exhausted all mtd techniques\")\n",
    "                continue\n",
    "            i += 1\n",
    "\n",
    "            new_obs, reward, done = self.environment.step(action)\n",
    "            idx1 = -1 if obs[0, -1] in Behavior else -2\n",
    "            idx2 = -1 if new_obs[0, -1] in Behavior else -2\n",
    "            transition = (obs[:, :idx1], action, reward, new_obs[:, :idx2], done)\n",
    "            self.agent.replay_buffer.append(transition)\n",
    "\n",
    "            obs = new_obs\n",
    "            if done:\n",
    "                obs = self.environment.reset()\n",
    "                episode_action_memory = []\n",
    "\n",
    "    def train_agent(self, num_episodes, t_update_freq, verbose=False):\n",
    "        episode_returns, eps_history = [], []\n",
    "        step = 0\n",
    "        for i in range(num_episodes):\n",
    "            episode_return = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "            obs = self.environment.reset()\n",
    "            while not done:\n",
    "                idx1 = -1 if obs[0, -1] in Behavior else -2\n",
    "                action = self.agent.choose_action(obs[:, :idx1])\n",
    "                if action == -1:\n",
    "                    print(\"Agent exhausted all MTD techniques upon behavior: \", obs[0, -1])\n",
    "                    self.agent.episode_action_memory = set()\n",
    "                    done = True\n",
    "                    continue\n",
    "\n",
    "                new_obs, reward, done = self.environment.step(action)\n",
    "                idx2 = -1 if new_obs[0, -1] in Behavior else -2\n",
    "                episode_return += reward\n",
    "                self.agent.replay_buffer.append((obs[:, :idx1], action, reward,\n",
    "                                            new_obs[:, :idx2], done))\n",
    "                self.agent.reward_buffer.append(reward)\n",
    "                if done:\n",
    "                    self.agent.episode_action_memory = set()\n",
    "\n",
    "                self.agent.learn()\n",
    "                obs = new_obs\n",
    "\n",
    "                episode_steps += 1\n",
    "                # update target network\n",
    "                step += 1\n",
    "                if step % t_update_freq == 0:\n",
    "                    self.agent.update_target_network()\n",
    "\n",
    "                # if step % LOG_FREQ == 0:\n",
    "                # print(\"Episode: \", i, \"Step: \", step, \", Avg Reward: \", np.mean(agent.reward_buffer), \"epsilon: \", agent.epsilon)\n",
    "\n",
    "            self.episode_returns.append(episode_return / episode_steps)\n",
    "            avg_episode_return = np.mean(episode_returns[-10:])\n",
    "            self.eps_history.append(self.agent.epsilon)\n",
    "            \n",
    "            if verbose:\n",
    "                print('| agent %d' % self.agent.agent_id,\n",
    "                  '| episode ', i, '| episode_return %.2f' % episode_returns[-1],\n",
    "                  '| average episode_return %.2f' % avg_episode_return,\n",
    "                  '| epsilon %.2f' % self.agent.epsilon)\n",
    "            #if i >= num_episodes - 6:\n",
    "                #print(episode_returns[-10:])\n",
    "                \n",
    "            #self.episode_returns+=episode_returns\n",
    "            #self.eps_history+=eps_history\n",
    "        return episode_returns, eps_history\n",
    "        \n",
    "    def get_training_summary(self):\n",
    "        return self.episode_returns, self.eps_history\n",
    "    \n",
    "    def receive_weights(self, model_params):\n",
    "        \"\"\" Receive aggregated parameters, update model \"\"\"\n",
    "        #self.agent.load_state_dict(copy.deepcopy(model_params))\n",
    "        self.agent.update_weights(model_params)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.agent.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0a954f3-90e1-4f78-9f52-7de52f139a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "from typing import List, Dict\n",
    "import threading\n",
    "import numpy as np\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, global_agent: Agent, test_data, nr_rounds = NR_ROUNDS, parallelized=False, verbose=True):\n",
    "        self.clients = []\n",
    "        self.global_agent = global_agent\n",
    "        self.test_data = test_data\n",
    "        self.nr_rounds = nr_rounds\n",
    "        self.parallelized = parallelized\n",
    "        self.verbose = verbose\n",
    "      \n",
    "    def aggregate_weights(self):\n",
    "        print(\"=== AGGREGATING WEIGHTS ===\")\n",
    "        client_params = {client.client_id: client.get_weights() for client in self.clients}\n",
    "        new_params = copy.deepcopy(next(iter(client_params.values())))  # names\n",
    "        for name in new_params:\n",
    "            new_params[name] = torch.zeros(new_params[name].shape)\n",
    "        for client_id, params in client_params.items():\n",
    "            client_weight = 1/len(self.clients)\n",
    "            for name in new_params:\n",
    "                new_params[name] += params[name] * client_weight  # averaging\n",
    "        #set new parameters to global model\n",
    "        self.global_agent.update_weights(new_params)\n",
    "        #print(new_params)\n",
    "        return new_params\n",
    "         \n",
    "    def broadcast_weights(self):\n",
    "        \"\"\" Send to all clients \"\"\"\n",
    "        for client in self.clients:\n",
    "            client.receive_weights(self.global_agent.get_weights())\n",
    "\n",
    "    def add_client(self, client: Client):\n",
    "        self.clients.append(client)\n",
    "      \n",
    "    def training_dist(self, verbose=False):\n",
    "        for nr_round in range(self.nr_rounds):\n",
    "            if self.verbose:\n",
    "                print(f\">>> SERVER TRAINING ROUND {nr_round + 1}/{self.nr_rounds} <<<\")\n",
    "            for client in self.clients:\n",
    "                print(f\"> AGENT {client.client_id} TRAINING ROUND {nr_round + 1}/{self.nr_rounds} <\")\n",
    "                client.receive_weights(self.global_agent.get_weights())\n",
    "                \n",
    "                if self.parallelized:\n",
    "                    # Parallel training\n",
    "                    threads = []\n",
    "                    for client in self.clients:\n",
    "                        #client.agent.model.share_memory()\n",
    "                        t = threading.Thread(target=Client.train_agent, args=(client, 1000, 100))\n",
    "                        t.start()\n",
    "                        threads.append(t)\n",
    "                    for t in threads:\n",
    "                        t.join()\n",
    "                else:\n",
    "                    # Sequential training\n",
    "                    for client in self.clients:\n",
    "                        client.train_agent(1000, 100, verbose=verbose)\n",
    "            \n",
    "            self.aggregate_weights()\n",
    "            print(f\">> EVALUATION TRAINING ROUND {nr_round + 1}/{self.nr_rounds} <<\")\n",
    "            for client in self.clients:\n",
    "                print(f\"AGENT {client.client_id}\")\n",
    "                evaluate_agent(client.agent, self.test_data)\n",
    "            # When every client is done training then we can aggregate weights\n",
    "            print(f\"GLOBAL AGENT\")\n",
    "            evaluate_agent(self.global_agent, self.test_data)\n",
    "            \n",
    "    def plot_learning_curves(self):\n",
    "        for client in self.clients:\n",
    "            episode_returns, eps_history = client.get_training_summary()\n",
    "            plot_learning_curve(f\"{client.client_id}\", episode_returns, eps_history)\n",
    "     \n",
    "    '''\n",
    "    def compute_acc(self):\n",
    "        self.global_model.eval()\n",
    "\n",
    "        test_loss = 0\n",
    "        nr_correct = 0\n",
    "        len_test_data = 0\n",
    "        for attributes, labels in self.test_data:\n",
    "            labels.to(self.device)\n",
    "            features = attributes.float().to(self.device)\n",
    "            outputs = self.global_model(features).to(self.device)\n",
    "            # accuracy\n",
    "            if self.data == 'MNIST' or self.data == 'FEMNIST':\n",
    "                pred_labels = torch.argmax(outputs, dim=1).to(self.device)\n",
    "            elif self.data == 'MED':\n",
    "                pred_labels = torch.round(outputs).to(self.device)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            nr_correct += torch.eq(pred_labels.to(self.device), labels.to(self.device)).type(torch.uint8).sum().item()\n",
    "            len_test_data += len(attributes)\n",
    "            # loss\n",
    "            test_loss += self.criterion(outputs.to(self.device), labels.to(self.device))\n",
    "\n",
    "        return nr_correct / len_test_data, test_loss.item()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca0f7306-6808-436d-88f8-7dbe0d351500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum = 0\\n#agent.save_agent_state(0, f\"{experiment_base_dir}\")\\n\\nx = [i + 1 for i in range(N_EPISODES)]\\nfilename = f\"{experiment_base_dir}/mtd_agent_p1.pdf\"\\nplot_learning(x, episode_returns, eps_history, filename)\\n\\n# check predictions with dqn from trained and stored agent\\npretrained_agent = get_pretrained_agent(path=f\"{experiment_base_dir}/trained_models/agent_{num}.pth\",\\n                                        input_dims=environment_01.observation_space_size, n_actions=len(environment_01.actions),\\n                                        buffer_size=BUFFER_SIZE)\\n# check predictions with learnt dqn\\nevaluate_agent(pretrained_agent, test_data=test_data)\\nha\\n# check scaling if uncommented\\n# print(\"evaluate p1 agent on \\'real\\' decision and afterstate data:\")\\n# dtrain, dtest, atrain, atest = DataProvider.get_reduced_dimensions_with_pca_ds_as(DIMS,\\n#                                                                                   dir=f\"{experiment_base_dir}/\")\\n# evaluate_agent(agent=pretrained_agent, test_data=dtest)\\n# evaluate_agent_on_afterstates(agent=pretrained_agent, test_data=atest)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num = 0\n",
    "#agent.save_agent_state(0, f\"{experiment_base_dir}\")\n",
    "\n",
    "x = [i + 1 for i in range(N_EPISODES)]\n",
    "filename = f\"{experiment_base_dir}/mtd_agent_p1.pdf\"\n",
    "plot_learning(x, episode_returns, eps_history, filename)\n",
    "\n",
    "# check predictions with dqn from trained and stored agent\n",
    "pretrained_agent = get_pretrained_agent(path=f\"{experiment_base_dir}/trained_models/agent_{num}.pth\",\n",
    "                                        input_dims=environment_01.observation_space_size, n_actions=len(environment_01.actions),\n",
    "                                        buffer_size=BUFFER_SIZE)\n",
    "# check predictions with learnt dqn\n",
    "evaluate_agent(pretrained_agent, test_data=test_data)\n",
    "ha\n",
    "# check scaling if uncommented\n",
    "# print(\"evaluate p1 agent on 'real' decision and afterstate data:\")\n",
    "# dtrain, dtest, atrain, atest = DataProvider.get_reduced_dimensions_with_pca_ds_as(DIMS,\n",
    "#                                                                                   dir=f\"{experiment_base_dir}/\")\n",
    "# evaluate_agent(agent=pretrained_agent, test_data=dtest)\n",
    "# evaluate_agent_on_afterstates(agent=pretrained_agent, test_data=atest)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca0637c6-9328-4d39-9d84-801e2cc85a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jankreischer/Library/Mobile Documents/com~apple~CloudDocs/Master-Thesis/Code\n",
      ">>> SERVER TRAINING ROUND 1 <<<\n",
      "> AGENT 1 TRAINING ROUND 1 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jankreischer/opt/anaconda3/envs/FedRL-for-IT-Sec/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> AGENT 2 TRAINING ROUND 1 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">>> SERVER TRAINING ROUND 2 <<<\n",
      "> AGENT 1 TRAINING ROUND 2 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 2 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      "Total training time:  41.31780695915222\n"
     ]
    }
   ],
   "source": [
    "experiment_base_dir = \"experiments/jans_experiment_01\"\n",
    "\n",
    "seed_random()\n",
    "start = time()\n",
    "\n",
    "# read in all preprocessed data for a simulated, supervised environment to sample from\n",
    "train_data, test_data, _ = DataProvider.get_scaled_train_test_split(scaling_minmax=True, scale_normal_only=True)\n",
    "# train_data, test_data = DataProvider.get_reduced_dimensions_with_pca(DIMS, pi=PI, normal_only=True)\n",
    "environment_01 = SensorEnvironment(train_data)\n",
    "environment_02 = SensorEnvironment(train_data)\n",
    "\n",
    "#print(type(train_data))\n",
    "#print(train_data.keys())\n",
    "#print(\"---\")\n",
    "#print(len(train_data[Behavior.NORMAL]))\n",
    "#print(train_data[Behavior.NORMAL])\n",
    "#print(\"---\")\n",
    "#print(type(test_data))\n",
    "\n",
    "# The input size is the number of features \n",
    "#print(\"state size: \", environment_01.observation_space_size)\n",
    "\n",
    "global_agent = Agent(0, input_dims=environment_02.observation_space_size, n_actions=len(environment_02.actions), buffer_size=BUFFER_SIZE,\n",
    "                batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "agent_01 = Agent(1, input_dims=environment_01.observation_space_size, n_actions=len(environment_01.actions), buffer_size=BUFFER_SIZE,\n",
    "              batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "agent_02 = Agent(2, input_dims=environment_02.observation_space_size, n_actions=len(environment_02.actions), buffer_size=BUFFER_SIZE,\n",
    "                batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "server = Server(global_agent, nr_rounds=2, parallelized=False, verbose=True)\n",
    "\n",
    "client_01 = Client(1, agent_01, environment_01)\n",
    "client_02 = Client(2, agent_02, environment_02)\n",
    "# initialize memory replay buffer (randomly)\n",
    "client_01.init_replay_memory(MIN_REPLAY_SIZE)\n",
    "client_02.init_replay_memory(MIN_REPLAY_SIZE)\n",
    "\n",
    "server.add_client(client_01)\n",
    "server.add_client(client_02)\n",
    "\n",
    "server.training_dist(verbose=False)\n",
    "\n",
    "# main training\n",
    "#episode_returns, eps_history = client_01.learn_agent_offline(num_episodes=N_EPISODES, t_update_freq=TARGET_UPDATE_FREQ)\n",
    "\n",
    "end = time()\n",
    "print(\"Total training time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "91bf722c-e010-4108-920c-b360eaa19f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jankreischer/Library/Mobile Documents/com~apple~CloudDocs/Master-Thesis/Code\n",
      "Recognized Behaviours\n",
      "dict_keys([<Behavior.NORMAL: 'normal'>, <Behavior.RANSOMWARE_POC: 'ransomware_poc'>, <Behavior.ROOTKIT_BDVL: 'bdvl'>, <Behavior.ROOTKIT_BEURK: 'beurk'>, <Behavior.CNC_THETICK: 'the_tick'>, <Behavior.CNC_BACKDOOR_JAKORITAR: 'backdoor_jakoritar'>, <Behavior.CNC_OPT1: 'data_leak_1'>, <Behavior.CNC_OPT2: 'data_leak_2'>])\n",
      "Recognized Behaviours\n",
      "dict_keys([<Behavior.NORMAL: 'normal'>, <Behavior.RANSOMWARE_POC: 'ransomware_poc'>, <Behavior.ROOTKIT_BDVL: 'bdvl'>, <Behavior.ROOTKIT_BEURK: 'beurk'>, <Behavior.CNC_THETICK: 'the_tick'>, <Behavior.CNC_BACKDOOR_JAKORITAR: 'backdoor_jakoritar'>, <Behavior.CNC_OPT1: 'data_leak_1'>, <Behavior.CNC_OPT2: 'data_leak_2'>])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'input_dims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m environment_02 \u001b[38;5;241m=\u001b[39m SensorEnvironment(train_data)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#print(type(train_data))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print(train_data.keys())\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print(\"---\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# The input size is the number of features \u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(\"state size: \", environment_01.observation_space_size)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m global_agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment_02\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menvironment_02\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBUFFER_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGAMMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPSILON_START\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPSILON_END\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_dec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPSILON_DEC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m agent_01 \u001b[38;5;241m=\u001b[39m Agent(\u001b[38;5;241m1\u001b[39m, input_dims\u001b[38;5;241m=\u001b[39menvironment_01\u001b[38;5;241m.\u001b[39mobservation_space_size, n_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(environment_01\u001b[38;5;241m.\u001b[39mactions), buffer_size\u001b[38;5;241m=\u001b[39mBUFFER_SIZE,\n\u001b[1;32m     27\u001b[0m               batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, gamma\u001b[38;5;241m=\u001b[39mGAMMA, epsilon\u001b[38;5;241m=\u001b[39mEPSILON_START, eps_end\u001b[38;5;241m=\u001b[39mEPSILON_END, eps_dec\u001b[38;5;241m=\u001b[39mEPSILON_DEC)\n\u001b[1;32m     29\u001b[0m agent_02 \u001b[38;5;241m=\u001b[39m Agent(\u001b[38;5;241m2\u001b[39m, input_dims\u001b[38;5;241m=\u001b[39menvironment_02\u001b[38;5;241m.\u001b[39mobservation_space_size, n_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(environment_02\u001b[38;5;241m.\u001b[39mactions), buffer_size\u001b[38;5;241m=\u001b[39mBUFFER_SIZE,\n\u001b[1;32m     30\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, gamma\u001b[38;5;241m=\u001b[39mGAMMA, epsilon\u001b[38;5;241m=\u001b[39mEPSILON_START, eps_end\u001b[38;5;241m=\u001b[39mEPSILON_END, eps_dec\u001b[38;5;241m=\u001b[39mEPSILON_DEC)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'input_dims'"
     ]
    }
   ],
   "source": [
    "experiment_base_dir = \"experiments/jans_experiment_01\"\n",
    "\n",
    "seed_random()\n",
    "start = time()\n",
    "\n",
    "# read in all preprocessed data for a simulated, supervised environment to sample from\n",
    "train_data, test_data, _ = DataProvider.get_scaled_train_test_split(scaling_minmax=True, scale_normal_only=True)\n",
    "# train_data, test_data = DataProvider.get_reduced_dimensions_with_pca(DIMS, pi=PI, normal_only=True)\n",
    "environment_01 = SensorEnvironment(train_data)\n",
    "environment_02 = SensorEnvironment(train_data)\n",
    "\n",
    "#print(type(train_data))\n",
    "#print(train_data.keys())\n",
    "#print(\"---\")\n",
    "#print(len(train_data[Behavior.NORMAL]))\n",
    "#print(train_data[Behavior.NORMAL])\n",
    "#print(\"---\")\n",
    "#print(type(test_data))\n",
    "\n",
    "# The input size is the number of features \n",
    "#print(\"state size: \", environment_01.observation_space_size)\n",
    "\n",
    "global_agent = Agent(0, input_dims=environment_02.observation_space_size, n_actions=len(environment_02.actions), buffer_size=BUFFER_SIZE,\n",
    "                batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "agent_01 = Agent(1, input_dims=environment_01.observation_space_size, n_actions=len(environment_01.actions), buffer_size=BUFFER_SIZE,\n",
    "              batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "agent_02 = Agent(2, input_dims=environment_02.observation_space_size, n_actions=len(environment_02.actions), buffer_size=BUFFER_SIZE,\n",
    "                batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "server = Server(global_agent, nr_rounds=2, parallelized=True, verbose=True)\n",
    "\n",
    "client_01 = Client(1, agent_01, environment_01)\n",
    "client_02 = Client(2, agent_02, environment_02)\n",
    "# initialize memory replay buffer (randomly)\n",
    "client_01.init_replay_memory(MIN_REPLAY_SIZE)\n",
    "client_02.init_replay_memory(MIN_REPLAY_SIZE)\n",
    "\n",
    "server.add_client(client_01)\n",
    "server.add_client(client_02)\n",
    "\n",
    "server.training_dist(verbose=False)\n",
    "\n",
    "# main training\n",
    "#episode_returns, eps_history = client_01.learn_agent_offline(num_episodes=N_EPISODES, t_update_freq=TARGET_UPDATE_FREQ)\n",
    "\n",
    "end = time()\n",
    "print(\"Total training time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63f1f5e6-593d-418a-9949-6b1072c35151",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'server' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mserver\u001b[49m\u001b[38;5;241m.\u001b[39mplot_learning_curves()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'server' is not defined"
     ]
    }
   ],
   "source": [
    "server.plot_learning_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0567c0e7-547d-463d-8d65-c7abe2d1aa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jankreischer/Library/Mobile Documents/com~apple~CloudDocs/Master-Thesis/Code\n",
      "Recognized Behaviours\n",
      "dict_keys([<Behavior.NORMAL: 'normal'>, <Behavior.RANSOMWARE_POC: 'ransomware_poc'>, <Behavior.ROOTKIT_BDVL: 'bdvl'>, <Behavior.ROOTKIT_BEURK: 'beurk'>])\n",
      "Recognized Behaviours\n",
      "dict_keys([<Behavior.NORMAL: 'normal'>, <Behavior.CNC_THETICK: 'the_tick'>, <Behavior.CNC_BACKDOOR_JAKORITAR: 'backdoor_jakoritar'>, <Behavior.CNC_OPT1: 'data_leak_1'>, <Behavior.CNC_OPT2: 'data_leak_2'>])\n",
      ">>> SERVER TRAINING ROUND 1/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 1/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jankreischer/opt/anaconda3/envs/FedRL-for-IT-Sec/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/jankreischer/opt/anaconda3/envs/FedRL-for-IT-Sec/lib/python3.9/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> AGENT 2 TRAINING ROUND 1/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 1/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.66%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 1.44%      | rootkit_sanitizer              |\n",
      "| the_tick           | 98.69%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 98.83%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 99.73%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 2/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 2/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 2/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 2/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 99.04%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 6.55%      | rootkit_sanitizer              |\n",
      "| beurk              | 31.90%     | rootkit_sanitizer              |\n",
      "| the_tick           | 63.68%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 69.95%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 93.97%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 3/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 3/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 3/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 3/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.61%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 10.36%     | rootkit_sanitizer              |\n",
      "| beurk              | 30.18%     | rootkit_sanitizer              |\n",
      "| the_tick           | 77.01%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 74.06%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 94.23%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 4/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 4/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 4/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 4/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.71%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 96.89%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 11.43%     | rootkit_sanitizer              |\n",
      "| beurk              | 10.68%     | rootkit_sanitizer              |\n",
      "| the_tick           | 94.51%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 93.78%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 97.69%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 5/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 5/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 5/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 5/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.77%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.61%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 25.16%     | rootkit_sanitizer              |\n",
      "| beurk              | 11.91%     | rootkit_sanitizer              |\n",
      "| the_tick           | 93.60%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 92.96%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 96.81%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 6/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 6/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 6/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 6/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.39%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.23%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 47.30%     | rootkit_sanitizer              |\n",
      "| beurk              | 39.97%     | rootkit_sanitizer              |\n",
      "| the_tick           | 67.86%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 64.55%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 82.17%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 7/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 7/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 7/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 7/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.82%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.61%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 6.11%      | rootkit_sanitizer              |\n",
      "| beurk              | 7.94%      | rootkit_sanitizer              |\n",
      "| the_tick           | 96.73%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 96.01%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 99.56%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 8/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 8/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 8/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 8/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.98%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 97.00%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.77%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 41.28%     | rootkit_sanitizer              |\n",
      "| beurk              | 29.84%     | rootkit_sanitizer              |\n",
      "| the_tick           | 83.61%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 79.34%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 98.67%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 9/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 9/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 9/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 9/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 99.20%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.77%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 66.70%     | rootkit_sanitizer              |\n",
      "| beurk              | 55.44%     | rootkit_sanitizer              |\n",
      "| the_tick           | 58.33%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 45.89%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 73.91%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 10/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 10/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 10/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      ">> EVALUATION TRAINING ROUND 10/10 <<\n",
      "AGENT 1\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 98.77%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 100.00%    | rootkit_sanitizer              |\n",
      "| beurk              | 100.00%    | rootkit_sanitizer              |\n",
      "| the_tick           | 0.00%      | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 0.00%      | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 0.00%      | cnc_ip_shuffle                 |\n",
      "AGENT 2\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 0.00%      | rootkit_sanitizer              |\n",
      "| the_tick           | 100.00%    | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 100.00%    | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "GLOBAL AGENT\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.16%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 48.98%     | rootkit_sanitizer              |\n",
      "| beurk              | 33.74%     | rootkit_sanitizer              |\n",
      "| the_tick           | 76.75%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 73.71%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 99.11%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      "Total training time:  206.39240717887878\n"
     ]
    }
   ],
   "source": [
    "experiment_base_dir = \"experiments/jans_experiment_01\"\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DEC = 1e-4\n",
    "EPSILON_END = 0.01\n",
    "NR_ROUNDS = 10\n",
    "\n",
    "\n",
    "seed_random()\n",
    "start = time()\n",
    "\n",
    "full_train_data, full_test_data, _ = DataProvider.get_scaled_train_test_split(scaling_minmax=True, scale_normal_only=True)\n",
    "\n",
    "subset_1 = (Behavior.NORMAL, Behavior.RANSOMWARE_POC, Behavior.ROOTKIT_BDVL, Behavior.ROOTKIT_BEURK)\n",
    "subset_2 = (Behavior.NORMAL, Behavior.CNC_THETICK, Behavior.CNC_BACKDOOR_JAKORITAR, Behavior.CNC_OPT1, Behavior.CNC_OPT2)\n",
    "sub_train_1 = {x: full_train_data[x] for x in subset_1 if x in full_train_data}\n",
    "sub_train_2 = {x: full_train_data[x] for x in subset_2 if x in full_train_data}\n",
    "\n",
    "environment_01 = SensorEnvironment(sub_train_1)\n",
    "environment_02 = SensorEnvironment(sub_train_2)\n",
    "\n",
    "\n",
    "global_agent = Agent(0, input_dims=environment_02.observation_space_size, n_actions=len(environment_02.actions), buffer_size=BUFFER_SIZE,\n",
    "                batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "agent_01 = Agent(1, input_dims=environment_01.observation_space_size, n_actions=len(environment_01.actions), buffer_size=BUFFER_SIZE,\n",
    "              batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "agent_02 = Agent(2, input_dims=environment_02.observation_space_size, n_actions=len(environment_02.actions), buffer_size=BUFFER_SIZE,\n",
    "                batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "server = Server(global_agent, full_test_data, nr_rounds=NR_ROUNDS, parallelized=True, verbose=True)\n",
    "\n",
    "client_01 = Client(1, agent_01, environment_01)\n",
    "client_02 = Client(2, agent_02, environment_02)\n",
    "# initialize memory replay buffer (randomly)\n",
    "client_01.init_replay_memory(MIN_REPLAY_SIZE)\n",
    "client_02.init_replay_memory(MIN_REPLAY_SIZE)\n",
    "\n",
    "server.add_client(client_01)\n",
    "server.add_client(client_02)\n",
    "\n",
    "server.training_dist(verbose=False)\n",
    "\n",
    "end = time()\n",
    "print(\"Total training time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "546e4b25-03cd-423f-8e8e-b1913cb5b4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jankreischer/Library/Mobile Documents/com~apple~CloudDocs/Master-Thesis/Code\n",
      "Recognized Behaviours\n",
      "dict_keys([<Behavior.NORMAL: 'normal'>, <Behavior.RANSOMWARE_POC: 'ransomware_poc'>, <Behavior.ROOTKIT_BDVL: 'bdvl'>, <Behavior.ROOTKIT_BEURK: 'beurk'>])\n",
      "Recognized Behaviours\n",
      "dict_keys([<Behavior.NORMAL: 'normal'>, <Behavior.CNC_THETICK: 'the_tick'>, <Behavior.CNC_BACKDOOR_JAKORITAR: 'backdoor_jakoritar'>, <Behavior.CNC_OPT1: 'data_leak_1'>, <Behavior.CNC_OPT2: 'data_leak_2'>])\n",
      ">>> SERVER TRAINING ROUND 1/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 1/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jankreischer/opt/anaconda3/envs/FedRL-for-IT-Sec/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> AGENT 2 TRAINING ROUND 1/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.00%      | rootkit_sanitizer              |\n",
      "| beurk              | 2.74%      | rootkit_sanitizer              |\n",
      "| the_tick           | 99.09%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 97.89%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 99.20%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 2/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 2/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 2/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 0.18%      | rootkit_sanitizer              |\n",
      "| beurk              | 1.23%      | rootkit_sanitizer              |\n",
      "| the_tick           | 99.41%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 98.71%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 99.91%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 3/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 3/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 3/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 0.00%      | ransomware_file_extension_hide |\n",
      "| bdvl               | 23.91%     | rootkit_sanitizer              |\n",
      "| beurk              | 2.87%      | rootkit_sanitizer              |\n",
      "| the_tick           | 98.95%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 97.77%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 99.73%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 4/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 4/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 4/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n",
      "=== AGGREGATING WEIGHTS ===\n",
      "SERVER <--- WEIGHTS --- AGENT 1\n",
      "SERVER <--- WEIGHTS --- AGENT 2\n",
      "SERVER --- WEIGHTS ---> AGENT 0\n",
      "| Behavior           | Accuracy   | Objective                      |\n",
      "|--------------------+------------+--------------------------------|\n",
      "| ransomware_poc     | 97.75%     | ransomware_file_extension_hide |\n",
      "| bdvl               | 28.43%     | rootkit_sanitizer              |\n",
      "| beurk              | 31.07%     | rootkit_sanitizer              |\n",
      "| the_tick           | 82.36%     | cnc_ip_shuffle                 |\n",
      "| backdoor_jakoritar | 73.12%     | cnc_ip_shuffle                 |\n",
      "| data_leak_1        | 97.43%     | cnc_ip_shuffle                 |\n",
      "| data_leak_2        | 100.00%    | cnc_ip_shuffle                 |\n",
      ">>> SERVER TRAINING ROUND 5/10 <<<\n",
      "> AGENT 1 TRAINING ROUND 5/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 1\n",
      "> AGENT 2 TRAINING ROUND 5/10 <\n",
      "SERVER <--- WEIGHTS --- AGENT 0\n",
      "SERVER --- WEIGHTS ---> AGENT 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m server\u001b[38;5;241m.\u001b[39madd_client(client_01)\n\u001b[1;32m     41\u001b[0m server\u001b[38;5;241m.\u001b[39madd_client(client_02)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m end \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal training time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m-\u001b[39m start)\n",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36mServer.training_dist\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m# Sequential training\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients:\n\u001b[0;32m---> 61\u001b[0m             \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_weights()\n\u001b[1;32m     64\u001b[0m evaluate_agent(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_agent, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_data)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mClient.train_agent\u001b[0;34m(self, num_episodes, t_update_freq, verbose)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mepisode_action_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m obs \u001b[38;5;241m=\u001b[39m new_obs\n\u001b[1;32m     75\u001b[0m episode_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m b_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transitions])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m     88\u001b[0m b_rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([t[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transitions])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n\u001b[0;32m---> 89\u001b[0m b_new_obses \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m b_dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([t[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transitions])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n\u001b[1;32m     91\u001b[0m t_obses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(b_obses)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/FedRL-for-IT-Sec/lib/python3.9/site-packages/numpy/core/shape_base.py:471\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    469\u001b[0m sl \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m),) \u001b[38;5;241m*\u001b[39m axis \u001b[38;5;241m+\u001b[39m (_nx\u001b[38;5;241m.\u001b[39mnewaxis,)\n\u001b[1;32m    470\u001b[0m expanded_arrays \u001b[38;5;241m=\u001b[39m [arr[sl] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_base_dir = \"experiments/jans_experiment_01\"\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DEC = 1e-4\n",
    "EPSILON_END = 0.01\n",
    "NR_ROUNDS = 10\n",
    "\n",
    "\n",
    "seed_random()\n",
    "start = time()\n",
    "\n",
    "full_train_data, full_test_data, _ = DataProvider.get_scaled_train_test_split(scaling_minmax=True, scale_normal_only=True)\n",
    "\n",
    "subset_1 = (Behavior.NORMAL, Behavior.RANSOMWARE_POC, Behavior.ROOTKIT_BDVL, Behavior.ROOTKIT_BEURK)\n",
    "subset_2 = (Behavior.NORMAL, Behavior.CNC_THETICK, Behavior.CNC_BACKDOOR_JAKORITAR, Behavior.CNC_OPT1, Behavior.CNC_OPT2)\n",
    "sub_train_1 = {x: full_train_data[x] for x in subset_1 if x in full_train_data}\n",
    "sub_train_2 = {x: full_train_data[x] for x in subset_2 if x in full_train_data}\n",
    "\n",
    "environment_01 = SensorEnvironment(sub_train_1)\n",
    "environment_02 = SensorEnvironment(sub_train_2)\n",
    "\n",
    "\n",
    "global_agent = Agent(0, input_dims=environment_02.observation_space_size, n_actions=len(environment_02.actions), buffer_size=BUFFER_SIZE,\n",
    "                batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "agent_01 = Agent(1, input_dims=environment_01.observation_space_size, n_actions=len(environment_01.actions), buffer_size=BUFFER_SIZE,\n",
    "              batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "agent_02 = Agent(2, input_dims=environment_02.observation_space_size, n_actions=len(environment_02.actions), buffer_size=BUFFER_SIZE,\n",
    "                batch_size=BATCH_SIZE, lr=LEARNING_RATE, gamma=GAMMA, epsilon=EPSILON_START, eps_end=EPSILON_END, eps_dec=EPSILON_DEC)\n",
    "\n",
    "server = Server(global_agent, full_test_data, nr_rounds=NR_ROUNDS, parallelized=False, verbose=True)\n",
    "\n",
    "client_01 = Client(1, agent_01, environment_01)\n",
    "client_02 = Client(2, agent_02, environment_02)\n",
    "# initialize memory replay buffer (randomly)\n",
    "client_01.init_replay_memory(MIN_REPLAY_SIZE)\n",
    "client_02.init_replay_memory(MIN_REPLAY_SIZE)\n",
    "\n",
    "server.add_client(client_01)\n",
    "server.add_client(client_02)\n",
    "\n",
    "server.training_dist(verbose=False)\n",
    "\n",
    "end = time()\n",
    "print(\"Total training time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b285a51d-ab78-4b78-a028-54748eb2c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2109,  0.2877,  0.2205,  1.1144],\n",
      "        [-0.7684,  0.2043,  0.1833,  0.0543],\n",
      "        [ 2.3913, -0.1870,  0.4581,  1.1125],\n",
      "        [ 0.9539, -0.6145,  0.2496, -0.4977]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[0;32m----> 3\u001b[0m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "print(a)\n",
    "a[torch.argmax(a)]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FedRL-for-IT-Sec",
   "language": "python",
   "name": "fedrl-for-it-sec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
