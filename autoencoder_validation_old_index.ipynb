{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba26052c-2bb9-48c9-9565-ed139cc65688",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Investigating the State Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb2b2e7b-5fad-4d40-b163-163fc1e4eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d46ecf8-d59a-4a4d-ac54-aa66d170b4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jankreischer/Library/Mobile Documents/com~apple~CloudDocs/Master-Thesis/Code/state_anomaly_detection'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881d1679-57c3-45d7-82be-8b1ad97b9822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65bd5230-7298-4bb8-b7b8-fe3ec97fdfe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Behavior(Enum):\n",
    "    NORMAL = \"normal\"\n",
    "    ROOTKIT_BDVL = \"bdvl\"\n",
    "    ROOTKIT_BEURK = \"beurk\"\n",
    "    CNC_BACKDOOR_JAKORITAR = \"backdoor_jakoritar\"\n",
    "    CNC_THETICK = \"the_tick\"\n",
    "    CNC_OPT1 = \"data_leak_1\"\n",
    "    CNC_OPT2 = \"data_leak_2\"\n",
    "    RANSOMWARE_POC = \"ransomware_poc\"\n",
    "\n",
    "class MTDTechnique(Enum):\n",
    "    CNC_IP_SHUFFLE = \"cnc_ip_shuffle\"\n",
    "    ROOTKIT_SANITIZER = \"rootkit_sanitizer\"\n",
    "    RANSOMWARE_DIRTRAP = \"ransomware_directory_trap\"\n",
    "    RANSOMWARE_FILE_EXT_HIDE = \"ransomware_file_extension_hide\"\n",
    "    \n",
    "actions = [\n",
    "    MTDTechnique.CNC_IP_SHUFFLE,\n",
    "    MTDTechnique.ROOTKIT_SANITIZER,\n",
    "    MTDTechnique.RANSOMWARE_DIRTRAP,\n",
    "    MTDTechnique.RANSOMWARE_FILE_EXT_HIDE\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1706621-4201-4c95-ae21-24906498f1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relative_data_path = \"data\"\n",
    "\n",
    "# These samples were collected without only the attack influencing the device running\n",
    "# No extra MTD/Agent/Components were running on the respective ElectroSense device.\n",
    "behavior_datasets_file_paths: dict[Behavior, str] = {\n",
    "    Behavior.NORMAL: f\"{relative_data_path}/normal_expfs_online_samples_1_2022-08-20-09-16_5s.csv\",\n",
    "    Behavior.RANSOMWARE_POC: f\"{relative_data_path}/ransom_expfs_online_samples_1_2022-08-22-14-04_5s.csv\",\n",
    "    Behavior.ROOTKIT_BDVL: f\"{relative_data_path}/rootkit_bdvl_online_samples_1_2022-08-19-08-45_5s.csv\",\n",
    "    Behavior.ROOTKIT_BEURK: f\"{relative_data_path}/rootkit_beurk_online_samples_1_2022-09-01-18-12_5s.csv\",\n",
    "    Behavior.CNC_THETICK: f\"{relative_data_path}/cnc_thetick_online_samples_1_2022-08-30-16-11_5s.csv\",\n",
    "    Behavior.CNC_BACKDOOR_JAKORITAR: f\"{relative_data_path}/cnc_backdoor_jakoritar_new_online_samples_1_2022-09-02-09-19_5s.csv\",\n",
    "    Behavior.CNC_OPT1: f\"{relative_data_path}/cnc_opt_1_file_extr_online_samples_1_2022-09-24-22-08_5s.csv\",\n",
    "    Behavior.CNC_OPT2: f\"{relative_data_path}/cnc_opt_2_sysinfo_online_samples_1_2022-09-24-14-04_5s.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe001b1f-3817-41a0-b5ec-7654e804603b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: These columns are derived from data_availability.py -> check data\n",
    "\n",
    "constant_columns = ['cpuNice', 'cpuHardIrq', 'alarmtimer:alarmtimer_fired', 'tasksStopped',\n",
    "                    'alarmtimer:alarmtimer_start', 'cachefiles:cachefiles_create',\n",
    "                    'cachefiles:cachefiles_lookup', 'cachefiles:cachefiles_mark_active',\n",
    "                    'dma_fence:dma_fence_init', 'udp:udp_fail_queue_rcv_skb']\n",
    "# suspected cyclic/unstable\n",
    "correlated_features = ['tasks', 'tasksSleeping', 'tasksZombie', 'tasksRunning',\n",
    "                   'ramFree', 'ramUsed', 'ramCache', 'memAvail', 'numEncrypted',\n",
    "                   'iface0RX', 'iface0TX', 'iface1RX', 'iface1TX'\n",
    "                   ]\n",
    "\n",
    "# suspected undesirably reactive, distorting afterstates\n",
    "correlated_features += ['cpuSystem', 'block:block_dirty_buffer', 'cpuSoftIrq', 'cs', 'cpu-migrations',\n",
    "                    'irq:softirq_entry', 'kmem:kmem_cache_alloc', 'kmem:kmem_cache_free',\n",
    "                    'random:urandom_read', 'raw_syscalls:sys_enter', 'raw_syscalls:sys_exit',\n",
    "                    'sched:sched_switch', 'sched:sched_wakeup', 'skb:consume_skb', 'timer:hrtimer_start',\n",
    "                    'writeback:global_dirty_state']\n",
    "\n",
    "correlated_features += ['cpuIdle', 'cpuIowait', 'block:block_bio_backmerge', 'block:block_touch_buffer', 'clk:clk_set_rate',\n",
    "                    'irq:irq_handler_entry', 'jbd2:jbd2_start_commit', 'kmem:mm_page_alloc', 'kmem:mm_page_free',\n",
    "                    'preemptirq:irq_enable', 'sock:inet_sock_set_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd2e03a8-7611-441b-b24e-ca436a7656cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_status_columns = [\"time\", \"timestamp\", \"seconds\"]\n",
    "\n",
    "\n",
    "def get_filtered_df(path, \n",
    "                    filter_suspected_external_events=True,\n",
    "                    start_index=10,\n",
    "                    end_index=-1,\n",
    "                    filter_constant_columns=True,\n",
    "                    filter_outliers=True,\n",
    "                    filter_correlated_features=False):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    if filter_suspected_external_events:\n",
    "        # filter first hour of samples: 3600s / 50s = 72\n",
    "        # and drop last measurement due to the influence of logging in, respectively out of the server\n",
    "        df = df.iloc[start_index:end_index]\n",
    "\n",
    "    # filter for measurements where the device was connected\n",
    "    df = df[df['connectivity'] == 1]\n",
    "\n",
    "    # Remove model-irrelevant time status columns\n",
    "    df = df.drop(time_status_columns, axis=1)\n",
    "\n",
    "    if filter_outliers:\n",
    "        # drop outliers per measurement, indicated by (absolute z score) > 3\n",
    "        df = df[(np.nan_to_num(np.abs(stats.zscore(df))) < 3).all(axis=1)]\n",
    "        \n",
    "    if filter_constant_columns:\n",
    "        df = df.drop(constant_columns, axis=1)\n",
    "\n",
    "    if filter_correlated_features:\n",
    "        df = df.drop(correlated_features, axis=1)\n",
    "\n",
    "    assert df.isnull().values.any() == False, \"behavior data should not contain NaN values\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30a0ec9a-8c1a-484f-8f39-23483d54a826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_train_split_for_outliers(df: pd.DataFrame, b: Behavior, split=0.8):\n",
    "    df = df[df[\"attack\"] == b].drop([\"attack\"], axis=1)\n",
    "    df_test = df.sample(frac=1 - split)  # .reset_index(drop=True)\n",
    "    df_train = pd.concat([df, df_test]).drop_duplicates(keep=False)\n",
    "    train_filtered = df_train[(np.nan_to_num(np.abs(stats.zscore(df_train))) < 3).all(axis=1)]\n",
    "    train_filtered[\"attack\"] = b\n",
    "    df_test[\"attack\"] = b\n",
    "    train_filtered = train_filtered.to_numpy()\n",
    "    df_test = df_test.to_numpy()\n",
    "    return train_filtered, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bf10d19-f1d3-42fc-a143-6eb1a3280114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_no_mtd_behavior_data(filter_suspected_external_events=True,\n",
    "                               filter_constant_columns=True,\n",
    "                               filter_outliers=True,\n",
    "                               filter_correlated_features=False,\n",
    "                               ) -> dict[Behavior, np.ndarray]:\n",
    "    # print(os.getcwd())\n",
    "    full_df = pd.DataFrame()\n",
    "    for attack in behavior_datasets_file_paths:\n",
    "        print(f\"getting {attack}\")\n",
    "        df = get_filtered_df(behavior_datasets_file_paths[attack],\n",
    "                                            filter_suspected_external_events=filter_suspected_external_events,\n",
    "                                            start_index=50,\n",
    "                                            filter_constant_columns=filter_constant_columns,\n",
    "                                            filter_outliers=filter_outliers,\n",
    "                                            filter_correlated_features=filter_correlated_features)\n",
    "        df['attack'] = attack\n",
    "        full_df = pd.concat([full_df, df])\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "429a53ac-363e-42c5-9b70-f2494f1fda00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scaled_train_test_split(split=0.8, scaling_minmax=True, scale_normal_only=True, filter_outliers=True):\n",
    "    \"\"\"\n",
    "    Method returns dictionaries mapping behaviors to scaled train and test data, as well as the scaler used\n",
    "    Either decision states or raw behaviors can be utilized (decision flag) as no combinations\n",
    "    with mtd need to be considered\n",
    "    \"\"\"\n",
    "    #print(os.getcwd())\n",
    "    rdf = parse_no_mtd_behavior_data(filter_outliers=filter_outliers)\n",
    "    print(f\"type(rdf): {type(rdf)}\")\n",
    "    print(f\"rdf.columns: {rdf.columns}; {len(rdf.columns)}\")\n",
    "    # take split of all behaviors, concat, calc scaling, scale both train and test split\n",
    "    train_filtered, df_test = filter_train_split_for_outliers(rdf, Behavior.NORMAL, split)\n",
    "    print(f\"type(train_filtered): {type(train_filtered)}\")\n",
    "    print(f\"type(df_test): {type(df_test)}\")\n",
    "\n",
    "    # get behavior dicts for train and test\n",
    "    train_bdata = {}\n",
    "    test_bdata = {}\n",
    "    for b in rdf[\"attack\"].unique():\n",
    "        dtrain_filtered, df_test = filter_train_split_for_outliers(rdf, b, split)\n",
    "        train_bdata[b] = dtrain_filtered\n",
    "        test_bdata[b] = df_test\n",
    "        if b != Behavior.NORMAL and not scale_normal_only:\n",
    "            train_filtered = np.vstack((train_filtered, train_bdata[b]))\n",
    "\n",
    "    # fit scaler on either just normal data (if scale_normal_only), or all training data combined\n",
    "    scaler = StandardScaler() if not scaling_minmax else MinMaxScaler()\n",
    "    print(f\"Using scaler {scaler}\")\n",
    "    scaler.fit(train_filtered[:, :-1])\n",
    "\n",
    "    # get behavior dicts for scaled train and test data\n",
    "    scaled_train = {}\n",
    "    scaled_test = {}\n",
    "    for b, d in train_bdata.items():\n",
    "        scaled_train[b] = np.hstack((scaler.transform(d[:, :-1]), np.expand_dims(d[:, -1], axis=1)))\n",
    "        scaled_test[b] = np.hstack(\n",
    "            (scaler.transform(test_bdata[b][:, :-1]), np.expand_dims(test_bdata[b][:, -1], axis=1)))\n",
    "\n",
    "    # return also scaler in case of using the agent for online scaling\n",
    "    return scaled_train, scaled_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59969025-d608-4e39-b979-24e17d9c67b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(data, split=0.8):\n",
    "    row = int(len(data) * split)\n",
    "    X_train = data[:row, :-1].astype(np.float32)\n",
    "    X_valid = data[row:, :-1].astype(np.float32)\n",
    "    return X_train, X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7036512b-3624-4fd9-a274-3c2ecc1e6b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_test_dataset(test_data):\n",
    "    test_data_dict = {}\n",
    "\n",
    "    for behavior, behavior_data in test_data.items():\n",
    "        if behavior == Behavior.NORMAL:\n",
    "            behavior_data = behavior_data[:2800]\n",
    "            #continue\n",
    "        else:\n",
    "            behavior_data = behavior_data[:400]\n",
    "\n",
    "        test_data_dict[behavior] = behavior_data\n",
    "\n",
    "    return test_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f77fed7-ded2-4894-8e42-f9fb6d27ead3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting Behavior.NORMAL\n",
      "getting Behavior.RANSOMWARE_POC\n",
      "getting Behavior.ROOTKIT_BDVL\n",
      "getting Behavior.ROOTKIT_BEURK\n",
      "getting Behavior.CNC_THETICK\n",
      "getting Behavior.CNC_BACKDOOR_JAKORITAR\n",
      "getting Behavior.CNC_OPT1\n",
      "getting Behavior.CNC_OPT2\n",
      "type(rdf): <class 'pandas.core.frame.DataFrame'>\n",
      "rdf.columns: Index(['connectivity', 'cpuUser', 'cpuSystem', 'cpuIdle', 'cpuIowait',\n",
      "       'cpuSoftIrq', 'tasks', 'tasksRunning', 'tasksSleeping', 'tasksZombie',\n",
      "       'ramFree', 'ramUsed', 'ramCache', 'memAvail', 'iface0RX', 'iface0TX',\n",
      "       'iface1RX', 'iface1TX', 'numEncrypted', 'block:block_bio_backmerge',\n",
      "       'block:block_bio_remap', 'block:block_dirty_buffer',\n",
      "       'block:block_getrq', 'block:block_touch_buffer', 'block:block_unplug',\n",
      "       'clk:clk_set_rate', 'cpu-migrations', 'cs', 'fib:fib_table_lookup',\n",
      "       'filemap:mm_filemap_add_to_page_cache', 'gpio:gpio_value',\n",
      "       'ipi:ipi_raise', 'irq:irq_handler_entry', 'irq:softirq_entry',\n",
      "       'jbd2:jbd2_handle_start', 'jbd2:jbd2_start_commit', 'kmem:kfree',\n",
      "       'kmem:kmalloc', 'kmem:kmem_cache_alloc', 'kmem:kmem_cache_free',\n",
      "       'kmem:mm_page_alloc', 'kmem:mm_page_alloc_zone_locked',\n",
      "       'kmem:mm_page_free', 'kmem:mm_page_pcpu_drain', 'mmc:mmc_request_start',\n",
      "       'net:net_dev_queue', 'net:net_dev_xmit', 'net:netif_rx', 'page-faults',\n",
      "       'pagemap:mm_lru_insertion', 'preemptirq:irq_enable',\n",
      "       'qdisc:qdisc_dequeue', 'random:get_random_bytes',\n",
      "       'random:mix_pool_bytes_nolock', 'random:urandom_read',\n",
      "       'raw_syscalls:sys_enter', 'raw_syscalls:sys_exit', 'rpm:rpm_resume',\n",
      "       'rpm:rpm_suspend', 'sched:sched_process_exec',\n",
      "       'sched:sched_process_free', 'sched:sched_process_wait',\n",
      "       'sched:sched_switch', 'sched:sched_wakeup', 'signal:signal_deliver',\n",
      "       'signal:signal_generate', 'skb:consume_skb', 'skb:kfree_skb',\n",
      "       'skb:skb_copy_datagram_iovec', 'sock:inet_sock_set_state',\n",
      "       'task:task_newtask', 'tcp:tcp_destroy_sock', 'tcp:tcp_probe',\n",
      "       'timer:hrtimer_start', 'timer:timer_start',\n",
      "       'workqueue:workqueue_activate_work', 'writeback:global_dirty_state',\n",
      "       'writeback:sb_clear_inode_writeback', 'writeback:wbc_writepage',\n",
      "       'writeback:writeback_dirty_inode',\n",
      "       'writeback:writeback_dirty_inode_enqueue',\n",
      "       'writeback:writeback_dirty_page',\n",
      "       'writeback:writeback_mark_inode_dirty',\n",
      "       'writeback:writeback_pages_written', 'writeback:writeback_single_inode',\n",
      "       'writeback:writeback_write_inode', 'writeback:writeback_written',\n",
      "       'attack'],\n",
      "      dtype='object'); 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/42fl0kws5cs5j84mbf8mbhbh0000gn/T/ipykernel_22746/2293243636.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_filtered[\"attack\"] = b\n",
      "/var/folders/8v/42fl0kws5cs5j84mbf8mbhbh0000gn/T/ipykernel_22746/2293243636.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_filtered[\"attack\"] = b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train_filtered): <class 'numpy.ndarray'>\n",
      "type(df_test): <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/42fl0kws5cs5j84mbf8mbhbh0000gn/T/ipykernel_22746/2293243636.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_filtered[\"attack\"] = b\n",
      "/var/folders/8v/42fl0kws5cs5j84mbf8mbhbh0000gn/T/ipykernel_22746/2293243636.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_filtered[\"attack\"] = b\n",
      "/var/folders/8v/42fl0kws5cs5j84mbf8mbhbh0000gn/T/ipykernel_22746/2293243636.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_filtered[\"attack\"] = b\n",
      "/var/folders/8v/42fl0kws5cs5j84mbf8mbhbh0000gn/T/ipykernel_22746/2293243636.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_filtered[\"attack\"] = b\n",
      "/var/folders/8v/42fl0kws5cs5j84mbf8mbhbh0000gn/T/ipykernel_22746/2293243636.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_filtered[\"attack\"] = b\n",
      "/var/folders/8v/42fl0kws5cs5j84mbf8mbhbh0000gn/T/ipykernel_22746/2293243636.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_filtered[\"attack\"] = b\n",
      "/var/folders/8v/42fl0kws5cs5j84mbf8mbhbh0000gn/T/ipykernel_22746/2293243636.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_filtered[\"attack\"] = b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scaler MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data, _ = get_scaled_train_test_split(scaling_minmax=True,\n",
    "                                                                         scale_normal_only=True)\n",
    "normal_data = training_data[Behavior.NORMAL]\n",
    "threshold = int(len(normal_data) * 0.5)\n",
    "\n",
    "training_data[Behavior.NORMAL] = normal_data[:threshold]\n",
    "\n",
    "ae_training_data = normal_data[threshold:]  # use remaining samples for autoencoder\n",
    "ae_training_x, ae_valid_x = split_data(ae_training_data)\n",
    "\n",
    "N_FEATURES = normal_data.shape[1] -1\n",
    "flattend_test_data = np.empty([0, N_FEATURES+1])\n",
    "for behavior, behavior_data in test_data.items():\n",
    "    if behavior == Behavior.NORMAL:\n",
    "        NR_SAMPLES = 2800\n",
    "        behavior_data[:, -1] =  0\n",
    "    else:\n",
    "        NR_SAMPLES = 400\n",
    "        behavior_data[:, -1] = 1\n",
    "    #y_true = np.array([0 if behavior == Behavior.NORMAL else 1] * NR_SAMPLES)\n",
    "    \n",
    "    flattend_test_data = np.concatenate((flattend_test_data, behavior_data[:NR_SAMPLES]), axis=0)\n",
    "\n",
    "ae_test_x = flattend_test_data[:,:-1]\n",
    "ae_test_y = flattend_test_data[:,-1].astype(int)\n",
    "\n",
    "evaluation_data = {}\n",
    "for behavior, behavior_data in training_data.items():\n",
    "    if behavior == Behavior.NORMAL:\n",
    "        evaluation_data[behavior] = behavior_data[:2800]\n",
    "    else:\n",
    "        evaluation_data[behavior] = behavior_data[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "902df7ba-b167-4e20-aee4-d1b136894dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initial_autoencoder_architecture(n_features):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(n_features, 23),\n",
    "        nn.BatchNorm1d(23),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(23, 11),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(11, 23),\n",
    "        nn.BatchNorm1d(23),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(23, n_features,),\n",
    "        nn.GELU()\n",
    "    )\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec003ec5-0b7d-4ea7-a548-801959b6b5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, model, X_valid, X_test, y_test, evaluation_data, n_std=20, activation_function=torch.nn.ReLU(), batch_size: int = 64, verbose=False):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        validation_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(X_valid).type(torch.float),\n",
    "\n",
    "        )\n",
    "        self.validation_data_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        self.evaluation_data = evaluation_data\n",
    "\n",
    "        n_features = X_test.shape[1]\n",
    "        \n",
    "        self.model = model\n",
    "        self.threshold = None\n",
    "        self.loss_mean = None\n",
    "        self.loss_standard_deviation = None\n",
    "        \n",
    "        self.verbose = verbose\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    \n",
    "    def pretrain(self, X_train, optimizer=torch.optim.SGD, loss_function=torch.nn.MSELoss(reduction='sum'), num_epochs: int = 15, batch_size=64, verbose=False):\n",
    "        \n",
    "        training_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(X_train).type(torch.float),\n",
    "        )\n",
    "        training_data_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        epoch_losses = []\n",
    "        #for e in tqdm(range(num_epochs), unit=\"epoch\", leave=False):\n",
    "        for e in range(num_epochs):\n",
    "            self.train()\n",
    "            current_losses = []\n",
    "            for batch_index, (inputs,) in enumerate(training_data_loader):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = loss_function(inputs, outputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                current_losses.append(loss.item())\n",
    "            \n",
    "            epoch_losses.append(np.average(current_losses))\n",
    "            if verbose:\n",
    "                print(f'Training Loss in epoch {e + 1}: {epoch_losses[e]}')\n",
    "            \n",
    "        self.analyze_loss()\n",
    "\n",
    "    '''\n",
    "    This function uses normal data samles \n",
    "    after training the autoencoder to determine\n",
    "    values that can be considered normal\n",
    "    for the reconstruction loss based on normal samples\n",
    "    '''\n",
    "    def analyze_loss(self):\n",
    "        losses = []\n",
    "        \n",
    "        self.eval() \n",
    "        with torch.no_grad():\n",
    "            loss_function = torch.nn.MSELoss(reduction='sum')\n",
    "            for batch_index, (inputs,) in enumerate(self.validation_data_loader):\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = loss_function(inputs, outputs)\n",
    "                losses.append(loss.item())\n",
    "        \n",
    "        losses = np.array(losses)\n",
    "\n",
    "        self.loss_mean = losses.mean()\n",
    "        self.loss_standard_deviation = losses.std()\n",
    "\n",
    "        \n",
    "    def predict(self, x, n_std=3):\n",
    "        test_data = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(x).type(torch.float32)\n",
    "        )\n",
    "        test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "        all_predictions = torch.tensor([])  # .cuda()\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            ae_loss = torch.nn.MSELoss(reduction=\"sum\")\n",
    "            for idx, (batch_x,) in enumerate(test_data_loader):\n",
    "                model_predictions = self.forward(batch_x)\n",
    "                model_predictions = ae_loss(model_predictions, batch_x).unsqueeze(0)  # unsqueeze as batch_size set to 1\n",
    "                all_predictions = torch.cat((all_predictions, model_predictions))\n",
    "\n",
    "        threshold = self.loss_mean + n_std * self.loss_standard_deviation\n",
    "        all_predictions = (all_predictions > threshold).type(torch.long)\n",
    "        return all_predictions.flatten()\n",
    "    \n",
    "    \n",
    "    def predict_deviation(self, x):\n",
    "        test_data = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(x).type(torch.float32)\n",
    "        )\n",
    "        test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "        prediction_errors = torch.tensor([])\n",
    "        loss_function = torch.nn.MSELoss(reduction=\"sum\")\n",
    "        \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for batch_index, (inputs,) in enumerate(test_data_loader):\n",
    "                prediction = self.forward(inputs)\n",
    "                prediction_error = loss_function(inputs, prediction).unsqueeze(0)  # unsqueeze as batch_size set to 1\n",
    "                prediction_errors = torch.cat((prediction_errors, prediction_error))\n",
    "\n",
    "        return prediction_errors\n",
    "    \n",
    "    \n",
    "    def score(self):\n",
    "        n_std, accuracy = self.accuracy_score(None, None)\n",
    "        if self.verbose:\n",
    "            print(f\"Highest validation accuracy achieved {accuracy:.2f} with n_std={n_std}\")\n",
    "            self.evaluate(n_std)\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def accuracy_score(self, X, y):\n",
    "        #if not self.threshold:\n",
    "        #loss_mean, loss_standard_deviation = self.analyze_loss(X)\n",
    "        #n_stds = np.arange(0.1, 3, 0.1)\n",
    "        if self.loss_mean == None or self.loss_standard_deviation == None:\n",
    "              #print(\"accuracy_score_optimized > accurcy_loss()\")\n",
    "              self.analyze_loss()\n",
    "    \n",
    "        best_accuracy = 0\n",
    "        best_n_std = 0\n",
    "        #accuracies = []\n",
    "        y_dev = self.predict_deviation((self.X_test).astype(np.float32))\n",
    "        for n_std in self.n_stds:\n",
    "            y_true = self.y_test\n",
    "            threshold = self.loss_mean + n_std * self.loss_standard_deviation\n",
    "            y_pred = (y_dev > threshold).type(torch.long).detach().cpu().numpy()\n",
    "            \n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_n_std = n_std\n",
    "            #if self.verbose:\n",
    "            #    print(f\"n_std {n_std:.2f} -> accuracy: {accuracy}\")\n",
    "\n",
    "        return best_n_std, best_accuracy\n",
    "    \n",
    "    \n",
    "    def evaluate(self, n_std=3, tablefmt='pipe'):\n",
    "        results = []\n",
    "        labels= [0,1]\n",
    "        pos_label = 1\n",
    "            \n",
    "        y_true_total = np.empty([0])\n",
    "        y_pred_total = np.empty([0])\n",
    "        for behavior, data in self.evaluation_data.items():\n",
    "            y_true = np.array([0 if behavior == Behavior.NORMAL else 1] * len(data)).astype(int)\n",
    "            y_true_total = np.concatenate((y_true_total, y_true))\n",
    "            \n",
    "            print(f\"Using n_std: {n_std} as prediction threshold\")\n",
    "            y_pred = self.predict(data[:, :-1].astype(np.float32), n_std=n_std)\n",
    "            y_pred_total = np.concatenate((y_pred_total, y_pred))\n",
    "\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "            n_samples = len(y_true)\n",
    "            results.append([behavior.name.replace(\"_\", \"\\_\"), f'{(100 * accuracy):.2f}\\%', '\\\\notCalculated', '\\\\notCalculated', '\\\\notCalculated', str(n_samples)])\n",
    "\n",
    "        accuracy = accuracy_score(y_true_total, y_pred_total)\n",
    "        precision = precision_score(y_true_total, y_pred_total, average='binary', labels=labels, pos_label=pos_label, zero_division=1)\n",
    "        recall = recall_score(y_true_total, y_pred_total, average='binary', labels=labels, pos_label=pos_label, zero_division=1)\n",
    "        f1 = f1_score(y_true_total, y_pred_total, average='binary', labels=labels, pos_label=pos_label, zero_division=1)\n",
    "        n_samples = len(y_true_total)\n",
    "        results.append([\"GLOBAL\", f'{(100 * accuracy):.2f}\\%', f'{(100 * precision):.2f}\\%', f'{(100 * recall):.2f}\\%', f'{(100 * f1):.2f}\\%', n_samples])\n",
    "        print(\"-----------\")\n",
    "        print(tabulate(results, headers=[\"Behavior\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"\\#Samples\"], tablefmt=tablefmt)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90d2629e-5336-455b-bd72-7f1ce5aa0c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(initial_autoencoder_architecture(N_FEATURES), ae_valid_x, ae_test_x, ae_test_y, evaluation_data)\n",
    "autoencoder.pretrain(ae_training_x, optimizer=torch.optim.Adam(autoencoder.parameters(), lr=1e-4,  weight_decay=0.01), loss_function=RMSELoss(), num_epochs=100, batch_size=64, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf023c8f-e609-4b47-9772-e06034fa9882",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using n_std: 20 as prediction threshold\n",
      "Using n_std: 20 as prediction threshold\n",
      "Using n_std: 20 as prediction threshold\n",
      "Using n_std: 20 as prediction threshold\n",
      "Using n_std: 20 as prediction threshold\n",
      "Using n_std: 20 as prediction threshold\n",
      "Using n_std: 20 as prediction threshold\n",
      "Using n_std: 20 as prediction threshold\n",
      "-----------\n",
      "\\begin{tabular}{lllllr}\n",
      "\\hline\n",
      " Behavior                 & Accuracy   & Precision      & Recall         & F1-Score       &   \\#Samples \\\\\n",
      "\\hline\n",
      " NORMAL                   & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &        2532 \\\\\n",
      " RANSOMWARE\\_POC          & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &         400 \\\\\n",
      " ROOTKIT\\_BDVL            & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &         400 \\\\\n",
      " ROOTKIT\\_BEURK           & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &         400 \\\\\n",
      " CNC\\_THETICK             & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &         400 \\\\\n",
      " CNC\\_BACKDOOR\\_JAKORITAR & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &         400 \\\\\n",
      " CNC\\_OPT1                & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &         400 \\\\\n",
      " CNC\\_OPT2                & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &         400 \\\\\n",
      " GLOBAL                   & 100.00\\%   & 100.00\\%       & 100.00\\%       & 100.00\\%       &        5332 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "autoencoder.evaluate(n_std=20, tablefmt='latex_raw')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FedRL-for-IT-Sec",
   "language": "python",
   "name": "fedrl-for-it-sec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
