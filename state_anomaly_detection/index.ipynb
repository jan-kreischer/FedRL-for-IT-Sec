{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba26052c-2bb9-48c9-9565-ed139cc65688",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Investigating the State Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2b2e7b-5fad-4d40-b163-163fc1e4eb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jankreischer/opt/anaconda3/envs/FedRL-for-IT-Sec/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f95e578-2a3c-48a4-874d-e1f93ff2961a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Behavior(Enum):\n",
    "    NORMAL = \"normal\"\n",
    "    ROOTKIT_BDVL = \"bdvl\"\n",
    "    ROOTKIT_BEURK = \"beurk\"\n",
    "    CNC_BACKDOOR_JAKORITAR = \"backdoor_jakoritar\"\n",
    "    CNC_THETICK = \"the_tick\"\n",
    "    CNC_OPT1 = \"data_leak_1\"\n",
    "    CNC_OPT2 = \"data_leak_2\"\n",
    "    RANSOMWARE_POC = \"ransomware_poc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a60cb02-c4ed-45b7-9a84-e140425f714f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset-01.csv')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "39e0b3bc-16b4-4bd8-86ec-49769d526c42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full length of dataset: 59004\n"
     ]
    }
   ],
   "source": [
    "print(f\"Full length of dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36487322-41b1-458a-886b-c7c4e93f3212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping time status features\n",
    "time_status_columns = ['time', 'timestamp', 'seconds']\n",
    "try:\n",
    "    dataset.drop(time_status_columns, inplace=True, axis=1)\n",
    "except:\n",
    "    print(f\"All time status features {(time_status_columns)} are removed from the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1538a690-6a0c-4c03-b3b0-022f61c25f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fast_ml.feature_selection import get_constant_features\n",
    "\n",
    "# Removing constant features\n",
    "constant_features = set(get_constant_features(dataset, threshold=0.99, dropna=False)['Var'])\n",
    "try:\n",
    "    dataset.drop(constant_features, inplace=True, axis=1)\n",
    "except:\n",
    "    print(f\"All constant features {(constant_features)} are removed from the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a35a64e8-ef80-4cec-b47e-a3ea592270b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Scaling\n",
    "fit_normal_behavior_only = True\n",
    "standard_scaling = False\n",
    "if standard_scaling:\n",
    "    scaler = StandardScaler()\n",
    "else:\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "print(f\"Using {scaler}\")\n",
    "\n",
    "if fit_normal_behavior_only: \n",
    "    scaler.fit(dataset[dataset['behavior'] == \"Behavior.NORMAL\"].values[:,:-1])\n",
    "else: \n",
    "    scaler.fit(dataset.values[:,:-1])\n",
    "\n",
    "scaled_dataset = pd.DataFrame(scaler.transform(dataset.values[:,:-1]), columns=dataset.columns.drop(\"behavior\"), index=dataset.index)\n",
    "scaled_dataset[\"behavior\"] = dataset[\"behavior\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a90a50c0-f0b0-4a60-bcc2-5fa14b640459",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14702, 85)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normal = scaled_dataset.loc[scaled_dataset['behavior'] == \"Behavior.NORMAL\"].drop([\"behavior\"],  axis=1).to_numpy()\n",
    "X_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d55da84b-ac0a-4cea-89ed-9f2b81446e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(n_features, 64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(64, 16),\n",
    "        nn.GELU(),\n",
    "        #nn.Linear(32, 16),\n",
    "        #nn.GELU(),\n",
    "        #nn.Linear(16, 32),\n",
    "        #nn.GELU(),\n",
    "        nn.Linear(16, 64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(64, n_features),\n",
    "        nn.GELU()\n",
    "    )\n",
    "        self.threshold = None\n",
    "        self.loss_mean = None\n",
    "        self.loss_standard_deviation = None\n",
    "        self.prediction_loss_function = torch.nn.MSELoss(reduction=\"sum\")\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    \n",
    "    def pretrain(self, X_normal, optimizer=torch.optim.SGD, loss_function=torch.nn.MSELoss(reduction='mean'), num_epochs: int = 15, batch_size=64, verbose=False):\n",
    "        threshold = int(0.5*len(X_normal))\n",
    "        X_train = X_normal[:threshold]\n",
    "        X_valid = X_normal[threshold:]\n",
    "        \n",
    "        training_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(X_train).type(torch.float),\n",
    "        )\n",
    "        training_data_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        epoch_losses = []\n",
    "        #for e in tqdm(range(num_epochs), unit=\"epoch\", leave=False):\n",
    "        for e in range(num_epochs):\n",
    "            self.train()\n",
    "            current_losses = []\n",
    "            for batch_index, (inputs,) in enumerate(training_data_loader):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = loss_function(inputs, outputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                current_losses.append(loss.item())\n",
    "            \n",
    "            epoch_losses.append(np.average(current_losses))\n",
    "            if verbose:\n",
    "                print(f'Training Loss in epoch {e + 1}: {epoch_losses[e]}')\n",
    "            \n",
    "        self.analyze_loss(X_valid)\n",
    "\n",
    "    '''\n",
    "    This function uses normal data samles \n",
    "    after training the autoencoder to determine\n",
    "    values that can be considered normal\n",
    "    for the reconstruction loss based on normal samples\n",
    "    '''\n",
    "    def analyze_loss(self, X_valid):\n",
    "        validation_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(X_valid).type(torch.float),\n",
    "\n",
    "        )\n",
    "        validation_data_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        self.eval() \n",
    "        with torch.no_grad():\n",
    "            for batch_index, (inputs,) in enumerate(validation_data_loader):\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.prediction_loss_function(inputs, outputs)\n",
    "                losses.append(loss.item())\n",
    "        \n",
    "        losses = np.array(losses)\n",
    "        self.loss_mean = losses.mean()\n",
    "        self.loss_standard_deviation = losses.std()\n",
    "\n",
    "    def predict(self, x, n_std=3):\n",
    "        test_data = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(x).type(torch.float32)\n",
    "        )\n",
    "        test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "        all_predictions = torch.tensor([])  # .cuda()\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (batch_x,) in enumerate(test_data_loader):\n",
    "                model_predictions = self.forward(batch_x)\n",
    "                model_predictions = self.prediction_loss_function(model_predictions, batch_x).unsqueeze(0)  # unsqueeze as batch_size set to 1\n",
    "                all_predictions = torch.cat((all_predictions, model_predictions))\n",
    "\n",
    "        threshold = self.loss_mean + n_std * self.loss_standard_deviation\n",
    "        all_predictions = (all_predictions > threshold).type(torch.long)\n",
    "        return all_predictions.flatten()\n",
    "    \n",
    "    \n",
    "    def predict_deviation(self, x):\n",
    "        test_data = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(x).type(torch.float32)\n",
    "        )\n",
    "        test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "        prediction_errors = torch.tensor([])\n",
    "        \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (inputs,) in enumerate(test_data_loader):\n",
    "                prediction = self.forward(inputs)\n",
    "                prediction_error = self.prediction_loss_function(inputs, prediction).unsqueeze(0)  # unsqueeze as batch_size set to 1\n",
    "                prediction_errors = torch.cat((prediction_errors, prediction_error))\n",
    "\n",
    "        return prediction_errors\n",
    "    \n",
    "    \n",
    "    def score(self):\n",
    "        n_std, accuracy = self.accuracy_score(None, None)\n",
    "        if self.verbose:\n",
    "            print(f\"Highest validation accuracy achieved {accuracy:.2f} with n_std={n_std}\")\n",
    "            self.evaluate(n_std)\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def accuracy_score(self, X, y):\n",
    "        #if not self.threshold:\n",
    "        #loss_mean, loss_standard_deviation = self.analyze_loss(X)\n",
    "        #n_stds = np.arange(0.1, 3, 0.1)\n",
    "        if self.loss_mean == None or self.loss_standard_deviation == None:\n",
    "              #print(\"accuracy_score_optimized > accurcy_loss()\")\n",
    "              self.analyze_loss()\n",
    "    \n",
    "        best_accuracy = 0\n",
    "        best_n_std = 0\n",
    "        #accuracies = []\n",
    "        y_dev = self.predict_deviation((self.X_test).astype(np.float32))\n",
    "        for n_std in self.n_stds:\n",
    "            y_true = self.y_test\n",
    "            threshold = self.loss_mean + n_std * self.loss_standard_deviation\n",
    "            y_pred = (y_dev > threshold).type(torch.long).detach().cpu().numpy()\n",
    "            \n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_n_std = n_std\n",
    "            #if self.verbose:\n",
    "            #    print(f\"n_std {n_std:.2f} -> accuracy: {accuracy}\")\n",
    "\n",
    "        return best_n_std, best_accuracy\n",
    "    \n",
    "    \n",
    "    def evaluate(self, evaluation_data, n_std=3, tablefmt='pipe'):\n",
    "        results = []\n",
    "        labels= [0,1]\n",
    "        pos_label = 1\n",
    "        \n",
    "        y_true_total = np.empty([0])\n",
    "        y_pred_total = np.empty([0])\n",
    "        for behavior in Behavior:\n",
    "            X_behavior = evaluation_data.loc[evaluation_data['behavior'] == f\"Behavior.{behavior.name}\"].drop([\"behavior\"],  axis=1).to_numpy()\n",
    "            y_true = np.array([0 if behavior == Behavior.NORMAL else 1] * len(X_behavior)).astype(int)\n",
    "            y_true_total = np.concatenate((y_true_total, y_true))\n",
    "            \n",
    "            #print(f\"Using n_std: {n_std} as prediction threshold\")\n",
    "            y_pred = self.predict(X_behavior.astype(np.float32), n_std=n_std)\n",
    "            y_pred_total = np.concatenate((y_pred_total, y_pred))\n",
    "\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "            n_samples = len(y_true)\n",
    "            results.append([behavior.name.replace(\"_\", \"\\_\"), f'{(100 * accuracy):.2f}\\%', '\\\\notCalculated', '\\\\notCalculated', '\\\\notCalculated', str(n_samples)])\n",
    "\n",
    "        accuracy = accuracy_score(y_true_total, y_pred_total)\n",
    "        precision = precision_score(y_true_total, y_pred_total, average='binary', labels=labels, pos_label=pos_label, zero_division=1)\n",
    "        recall = recall_score(y_true_total, y_pred_total, average='binary', labels=labels, pos_label=pos_label, zero_division=1)\n",
    "        f1 = f1_score(y_true_total, y_pred_total, average='binary', labels=labels, pos_label=pos_label, zero_division=1)\n",
    "        n_samples = len(y_true_total)\n",
    "        results.append([\"GLOBAL\", f'{(100 * accuracy):.2f}\\%', f'{(100 * precision):.2f}\\%', f'{(100 * recall):.2f}\\%', f'{(100 * f1):.2f}\\%', n_samples])\n",
    "        print(\"-----------\")\n",
    "        print(tabulate(results, headers=[\"Behavior\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"\\#Samples\"], tablefmt=tablefmt)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1e07e2fb-92fd-4bfa-9ac4-02b270504a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_FEATURES = X_normal.shape[1]\n",
    "autoencoder = AutoEncoder(N_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e35819ae-249c-40ed-812e-84a42e8c4ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder.pretrain(X_normal, optimizer=torch.optim.Adam(autoencoder.parameters(), lr=1e-4,  weight_decay=0.01), loss_function=RMSELoss(), num_epochs=100, batch_size=64, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f6774653-b07f-4100-85b9-979833cc5f99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "\\begin{tabular}{lllllr}\n",
      "\\hline\n",
      " Behavior                 & Accuracy   & Precision      & Recall         & F1-Score       &   \\#Samples \\\\\n",
      "\\hline\n",
      " NORMAL                   & 95.50\\%    & \\notCalculated & \\notCalculated & \\notCalculated &       14702 \\\\\n",
      " ROOTKIT\\_BDVL            & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &        5698 \\\\\n",
      " ROOTKIT\\_BEURK           & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &        7358 \\\\\n",
      " CNC\\_BACKDOOR\\_JAKORITAR & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &        4312 \\\\\n",
      " CNC\\_THETICK             & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &        7704 \\\\\n",
      " CNC\\_OPT1                & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &        5687 \\\\\n",
      " CNC\\_OPT2                & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &        4162 \\\\\n",
      " RANSOMWARE\\_POC          & 100.00\\%   & \\notCalculated & \\notCalculated & \\notCalculated &        9381 \\\\\n",
      " GLOBAL                   & 98.88\\%    & 98.53\\%        & 100.00\\%       & 99.26\\%        &       59004 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "evaluation_data = scaled_dataset\n",
    "autoencoder.evaluate(evaluation_data, n_std=0.7, tablefmt='latex_raw')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FedRL-for-IT-Sec",
   "language": "python",
   "name": "fedrl-for-it-sec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
